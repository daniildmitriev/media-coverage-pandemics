{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libs\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "# STTM lib from Github\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Custom python scripts for preprocessing, prediction and\n",
    "# visualization that I will define more in depth later\n",
    "#from preprocessing import tokenize\n",
    "#from topic_allocation import top_words, topic_attribution\n",
    "#from visualisation import plot_topic_notebook\n",
    "# Load the 20NewsGroups dataset from sklearn\n",
    "#cats = ['talk.politics.mideast', 'comp.windows.x', 'sci.space']\n",
    "#newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['date', 'hashtags', 'name', 'likes_count', 'replies_count', 'retweets_count', 'tweet','username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An earlier version of this article misstated the date of the U.S.’s first confirmed case of coronavirus through travel. It was Jan. 20, not Jan. 8. We've deleted a tweet that also included the error.  https://nyti.ms/39lcxWs \n"
     ]
    }
   ],
   "source": [
    "who_df = pd.read_csv('nytimes_covid.csv',encoding='utf-8')\n",
    "who_df.drop_duplicates(keep=\"first\", inplace=True)\n",
    "who_df = who_df[columns]\n",
    "who_df = who_df.set_index('date')\n",
    "who_df.index = pd.to_datetime(who_df.index, format=\"%Y-%m-%d\")\n",
    "\n",
    "print (who_df.tweet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-585bc7233165>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mstopwords_perso\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"said\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmy_stopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmy_stopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords_perso\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'set' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "#my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "#stopwords_perso=[\"said\"]\n",
    "my_stopwords=set(stopwords.words('english'))\n",
    "#my_stopwords.append(stopwords_perso)\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
    "\n",
    "# cleaning master function\n",
    "def clean_tweet(tweet, bigrams=False):\n",
    "    \n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "    \n",
    "    tweet_token_list = word_tokenize(tweet)\n",
    "    tweet_token_list= [stemmer.stem(word) for word in tweet_token_list if word not in my_stopwords]\n",
    "    \n",
    "#    tweet_token_list = [word for word in tweet.split(' ')\n",
    "#                            if word not in my_stopwords] # remove stopwords\n",
    "\n",
    "    \n",
    "#    tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
    "#                        for word in tweet_token_list] # apply word rooter\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "        \n",
    "   \n",
    "    #tweet = ' '.join(tweet_token_list)\n",
    "    for x in tweet_token_list:\n",
    "        x.replace(\"u\",\" \")\n",
    "        \n",
    "    return tweet_token_list\n",
    "\n",
    "\n",
    "\n",
    "who_df.tweet=who_df.tweet.apply(remove_links)\n",
    "who_df.tweet=who_df.tweet.apply(remove_users)\n",
    "who_df.tweet=who_df.tweet.apply(clean_tweet)\n",
    "\n",
    "\n",
    "who_df.tweet[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10848\n",
      "2991\n",
      "('Voc size:', 13688)\n",
      "('Number of documents:', 10848)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i=0\n",
    "# Input format for the model : list of strings (list of tokens)\n",
    "tweets=who_df[\"tweet\"].tolist()\n",
    "print len(tweets)\n",
    "for tweet in tweets:\n",
    "    for x in tweet:\n",
    "        if x=='u' or (x==u'\\u2019') or (x==u'\\u201c') or (x==u'\\u2014') or (x==u'\\u201d') : #remove some special characters and the years\n",
    "            tweet.remove(x)\n",
    "            i+=1\n",
    "print i   \n",
    "vocab = set(x for tweet in tweets for x in tweet)\n",
    "#vocab = set(x for tweet in tweets for x in tweet if u'\\x99' not in x)\n",
    "#vocab = [x.replace(u'\\x99', '') for x in vocab]\n",
    "#vocab = [x.replace(u'\\x98', '') for x in vocab]\n",
    "n_terms = len(vocab)\n",
    "\n",
    "print(\"Voc size:\", n_terms)\n",
    "print(\"Number of documents:\", len(tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 8598 clusters with 10 clusters populated\n",
      "In stage 1: transferred 4833 clusters with 10 clusters populated\n",
      "In stage 2: transferred 2845 clusters with 10 clusters populated\n",
      "In stage 3: transferred 1922 clusters with 10 clusters populated\n",
      "In stage 4: transferred 1415 clusters with 10 clusters populated\n"
     ]
    }
   ],
   "source": [
    "# Train a new model \n",
    "\n",
    "# Init of the Gibbs Sampling Dirichlet Mixture Model algorithm\n",
    "mgp = MovieGroupProcess(K=10, alpha=0.1, beta=0.1, n_iters=5)\n",
    "\n",
    "#vocab = set(x for tweet in tweets for x in tweet)\n",
    "n_terms = len(vocab)\n",
    "n_tweets = len(tweets)\n",
    "\n",
    "# Fit the model on the data given the chosen seeds\n",
    "y = mgp.fit(tweets, n_terms)\n",
    "\n",
    "# Save model\n",
    "with open('model_v2.model', \"wb\") as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of tweets per topic :', array([1245, 1142,  641, 1193, 1248, 1144, 1661,  682, 1355,  537]))\n",
      "********************\n",
      "('Most important clusters (by number of tweets inside):', array([6, 8, 4, 0, 3, 5, 1, 7, 2, 9]))\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "tweet_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of tweets per topic :', tweet_count)\n",
    "print('*'*20)\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = tweet_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of tweets inside):', top_index)\n",
    "print('*'*20)\n",
    "# Show the top 5 words in term frequency for each cluster \n",
    "#top_words(mgp.cluster_word_distribution, top_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print('Cluster %s : %s'%(cluster,sort_dicts))\n",
    "        print(' — — — — — — — — — ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 6 : [(u'\\u201d', 471), (u'coronaviru', 362), (u'new', 260), (u'write', 254), (u'one', 223)]\n",
      " — — — — — — — — — \n",
      "Cluster 8 : [(u'democrat', 498), (u'presidenti', 306), (u'day', 303), (u'sander', 255), (u'need', 249)]\n",
      " — — — — — — — — — \n",
      "Cluster 4 : [(u'presid', 755), (u'trump', 721), (u'impeach', 443), (u'senat', 271), (u'hous', 263)]\n",
      " — — — — — — — — — \n",
      "Cluster 0 : [(u'\\u201d', 436), (u'new', 186), (u'write', 164), (u'year', 161), (u'critic', 136)]\n",
      " — — — — — — — — — \n",
      "Cluster 3 : [(u'coronaviru', 826), (u'new', 347), (u'outbreak', 250), (u'case', 238), (u'china', 235)]\n",
      " — — — — — — — — — \n",
      "Cluster 5 : [(u'said', 305), (u'year', 235), (u'new', 233), (u'peopl', 222), (u'kill', 207)]\n",
      " — — — — — — — — — \n",
      "Cluster 1 : [(u'coronaviru', 247), (u'trump', 176), (u'presid', 164), (u'said', 153), (u'chines', 149)]\n",
      " — — — — — — — — — \n",
      "Cluster 7 : [(u'news', 99), (u'write', 95), (u'presid', 90), (u'coronaviru', 89), (u'\\u201d', 88)]\n",
      " — — — — — — — — — \n",
      "Cluster 2 : [(u'\\u201d', 142), (u'said', 116), (u'year', 104), (u'new', 103), (u'one', 53)]\n",
      " — — — — — — — — — \n",
      "Cluster 9 : [(u'said', 111), (u'\\u201d', 101), (u'presid', 69), (u'year', 60), (u'twitter', 60)]\n",
      " — — — — — — — — — \n"
     ]
    }
   ],
   "source": [
    "top_words(mgp.cluster_word_distribution, top_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
